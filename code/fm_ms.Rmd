---
title: "fm.ms"
author: "M. Florencia Miguel"
date: "14 de junio de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Initialize loading the required `R` packages.

```{r initialize, echo= T, message= FALSE, warning=FALSE}

library(ggplot2)
library(statnet)
library(igraph)
library(sna)
library(ergm)
library(ggnetwork)
library(GGally)
library(bipartite)
```

# NETWORKS

```{r input}
main<- read.csv("../data/interaction_matrix.csv", header= T, sep= ",",
                dec = ".", na.strings = "NA")
str(main)

```


```{r adj_mat, echo= T}

grazed<- main %>% 
           dplyr::filter(site=="grazed") %>%  
           dplyr::select(1,32:43) 


ungrazed<- main %>%  
          dplyr::filter(site=="ungrazed")%>% 
          dplyr::select(1,32:43)

#both<- main %>%  
 #         dplyr::select(1,32:43)

head(grazed); head(ungrazed)

# head(both)

         
```


```{r labels, echo= TRUE}

  # Labels for family:
 famlab<- c("Muridae","Muridae","Muridae","Caviidae","Caviidae","Canidae","Mephitidae","Dasypodidae","Teiidae","Ctenomyidae","Bovidae","Equidae")

 expfamlab<- c("Rod_Muridae","Rod_Muridae","Rod_Muridae","Rod_Caviidae","Rod_Caviidae","Car_Canidae","Car_Mephitidae","Xen_Dasypodidae","Lac_Teiidae","Rod_Ctenomyidae","Arty_Bovidae","Peri_Equidae")
  #
  
```
  
  
  ## Plotting the interaction networks by site.
  
  ## For each site, with node labels, weighted adjacency matrices.
 
```{r plots_by_site, fig.width=9, eval= T}
  # Plotting bipartite networks from adjacency matrix of two-mode network.
  # Using ggplot2 -----------------------------------------------------------
  # 
 require(ggnetwork)
  
  # Bipartite network initialization, starting from an adjacency matrix.
  # Adjacency matrix (wG and wUG) from dataframe (grazed and ungrazed).
 
 wG<-t(grazed[,2:13])
 colnames(wG)<- grazed[,1]
 netwG<-network::network(wG, matrix.type= 'bipartite', ignore.eval= T)
  

 wUG<-t(ungrazed[,2:13])
 colnames(wUG)<- ungrazed[,1]
 netwUG<-network::network(wUG, matrix.type= 'bipartite', ignore.eval= T)
 
  
  # Function to compute edge weights, scaled.
 edge.weights<- function(M, x = 10) {
      # Edge list and weights.
      M <- cbind(expand.grid(dimnames(M))[2:1], as.vector(M))
      # Discard null weights.
      M <- subset(M, M[, 3] != 0)
      M <- subset(M, M[, 3] != 0) 
      # Scaled weights.
      M.scaled <- x*log(M[, 3] + 1) / max(log(M[, 3] + 1))
      # Vector of edge weights.
      return(M.scaled)
 }

   
  # Weighted bipartite networks
  # 
 # GRAZED
 bipwG= network(wG,
                matrix.type = "bipartite",
                ignore.eval = FALSE,
                names.eval = "weights",
               modes = c("Frug", "Trees"))
 
  # set colors for each mode
#col = c("actor" = "grey", "event" = "gold")


G<-ggnet2(bipwG, size = 4 ,
        edge.size= edge.weights(wG, 5), edge.alpha= .25,
         label= T, label.size= 1.5,
         color= "mode", palette = "Set2",
         shape= "mode") 
G


 # UNGRAZED
bipwUG= network(wUG,
                matrix.type = "bipartite",
                ignore.eval = FALSE,
                names.eval = "weights",
               modes = c("Frug", "Trees"))
  # set colors for each mode
  #col = c("actor" = "grey", "event" = "gold")
  
 Ug<-ggnet2(bipwUG, node.size = 4,
        edge.size= edge.weights(wUG, 5), edge.alpha= .25,
         label= TRUE, label.size= 1.5,
         color= "mode", palette = "Set2",
         shape= "mode")
 Ug
 

```


```{r saving network plots}

ggsave("Grazed network.pdf", plot = last_plot(), device = "pdf", path = NULL,
  scale = 1, width = 16.6, height = 8, units = "cm",
  dpi = 300)

ggsave("Ungrazed network.pdf", plot = last_plot(), device = "pdf", path = NULL,
  scale = 1, width = 16.6, height = 8, units = "cm",
  dpi = 300)

```



# Plot bipartite network for all trees 

```{r plot_all_trees, fig.width=9, eval= T}

# Plotting bipartite networks from adjacency matrix of two-mode network.
  # Using ggplot2 -----------------------------------------------------------
  # 
 require(ggnetwork)
  
  # Bipartite network initialization, starting from an adjacency matrix.
  # Matrix from dataframe.
 

wboth<-t(both[,2:13])
 colnames(wboth)<- both[,1]
 netwboth<-network::network(both, matrix.type= 'bipartite', ignore.eval= T)
  
  # Function to compute edge weights, scaled.
 edge.weights<- function(M, x = 10) {
      # Edge list and weights.
      M <- cbind(expand.grid(dimnames(M))[2:1], as.vector(M))
      # Discard null weights.
      M <- subset(M, M[, 3] != 0)
      M <- subset(M, M[, 3] != 0) 
      # Scaled weights.
      M.scaled <- x*log(M[, 3] + 1) / max(log(M[, 3] + 1))
      # Vector of edge weights.
      return(M.scaled)
  }

 # Weighted bipartite networks
  # 
# Both land uses
 bipwboth= network(wboth,
                matrix.type = "bipartite",
                ignore.eval = FALSE,
                names.eval = "weights",
               modes = c("Frug", "Trees"))
 
  # set colors for each mode
  # col = c("actor" = "grey", "event" = "gold")
  
 ggnet2(bipwboth, node.size = 5,
        edge.size= edge.weights(wboth, 5), edge.alpha= .25,
         label= TRUE, label.size= 3,
         color= "mode", palette = "Set2",
         shape= "mode")
```

#Calculate Connectance, Modularity and Hamming Distance between networks
Bipartite provides functions to visualise webs and calculate a series of indices commonly used to describe pattern in (ecological) networks trees as columns (m) and frugivore species (n) as rows, n x m matrix.

### GRAZED

```{r indices, echo= TRUE, message= TRUE, warning= TRUE}

require(bipartite)

splevelG<-specieslevel(wG)# This gives you a lot of parameters level="higher" or level="lower"
# for each species and each Prosopis tree
splevelG

spG<-specieslevel(wG,level="higher")
spG

write.csv(spUG,file="spUG.csv")

sp<-specieslevel(wG,level="lower")
sp

spUG<-specieslevel(wUG, level= "higher") # This gives you a lot of parameters
# for each species and each Prosopis tree
splevelUG

networklevel(wG, index="ALLBUTDD", level="both", weighted=TRUE, 
   ISAmethod="Bluethgen",  SAmethod = "Bluethgen", extinctmethod = "r", 
   nrep = 100, CCfun=median, dist="horn", normalise=TRUE, empty.web=TRUE, 
   logbase="e", intereven="prod", H2_integer=TRUE, fcweighted=TRUE, 
   fcdist="euclidean", legacy=FALSE)

grouplevel(wG, index="ALLBUTDD", level="both", weighted=TRUE, empty.web=TRUE, 
dist="horn", CCfun=mean, logbase="e", normalise=TRUE,  extinctmethod="r", 
nrep=100, fcdist="euclidean", fcweighted=TRUE)

networklevel(wUG, index="ALLBUTDD", level="both", weighted=TRUE, 
   ISAmethod="Bluethgen",  SAmethod = "Bluethgen", extinctmethod = "r", 
   nrep = 100, CCfun=median, dist="horn", normalise=TRUE, empty.web=TRUE, 
   logbase="e", intereven="prod", H2_integer=TRUE, fcweighted=TRUE, 
   fcdist="euclidean", legacy=FALSE)

grouplevel(wUG, index="ALLBUTDD", level="both", weighted=TRUE, empty.web=TRUE, 
dist="horn", CCfun=mean, logbase="e", normalise=TRUE,  extinctmethod="r", 
nrep=100, fcdist="euclidean", fcweighted=TRUE)

```

```{r modularity, echo=TRUE, message= TRUE, warning= TRUE}

#Grazed sites
MG<- computeModules(wG, method="Beckett", deep= FALSE, deleteOriginalFiles= FALSE,
                             steps= 1000, tolerance= 1e-10, experimental= FALSE, 
                             forceLPA= FALSE)

MG  #0.5176449

listModuleInformation(MG)

printoutModuleInformation(MG)

plotModuleWeb(MG)


#Ungrazed sites
MUG<- computeModules(wUG, method="Beckett", deep=FALSE, deleteOriginalFiles=FALSE,
                             steps= 1000, tolerance= 1e-10, experimental= FALSE, 
                             forceLPA= FALSE)

MUG #0.4084378


listModuleInformation(MUG)

printoutModuleInformation(MUG)

plotModuleWeb(MUG)


#Mboth<- computeModules(wboth, method="Beckett", deep=FALSE, deleteOriginalFiles=FALSE,
                         #   steps= 1000, tolerance= 1e-10, experimental= FALSE, 
                         #    forceLPA= FALSE)

#Mboth

#listModuleInformation(Mboth)

#printoutModuleInformation(Mboth)

#plotModuleWeb(Mboth)

```


```{r save plotmodules, fig.width=9}

pdf('modulesgrazed.pdf', width = 60, height = 48,  pointsize = 60)
plotModuleWeb(MG)
dev.off()

pdf('modulesungrazed.pdf', width = 60, height = 48,  pointsize = 60)
plotModuleWeb(MUG)
dev.off()

```




```{r czvalues}

czGf<-czvalues(MG, weighted=TRUE, level="lower")
czGf

czGp<-czvalues(MG, weighted=TRUE, level="higher")
czGp

czUGf<-czvalues(MUG, weighted=TRUE, level="lower")
czUGf

czUGp<-czvalues(MUG, weighted=TRUE, level="higher")
czUGp

```


```{r mod_signif, echo=FALSE, message= TRUE, warning= TRUE}

#Grazed sites
# --------------------------------------------------------------------
# [Title]: Modularity analysis.
# [Date]: 11Jun2013     [Loc]: Sevilla
# Pedro Jordano.
# --------------------------------------------------------------------
## First version 11Jun2013 Revised DATE
# --------------------------------------------------------------------
# Interaction matrices to compute the modularity.
mymat<- wG                        # [Assign here]
#
# Batch to generate the null models for modularity M significance test. ----- 
# (for each matrix, @30 min for 100 null replicates)
require(bipartite)
TIME <- Sys.time()
# Modularity for observed matrix.
# Give here the mean observed modularity values.
mod_obs <- 0.5176449              # [Assign here]
#
Msig <- function (mat, mlike)  {
    require(bipartite)
    # mat is the input matrix for which M is tested
    # mlike is the observed mean M value
    nulls <- nullmodel(mymat, N=100, method=3)
    modules.nulls <- sapply(nulls, computeModules)
    like.nulls <- sapply(modules.nulls, function(x) x@likelihood)
    z <- (mlike - mean(like.nulls))/sd(like.nulls)
    p <- 2*pnorm(-abs(z))
    cat("\n\n","P value for modularity M= ", mod_obs, "\n", "\n\n",
        "zeta=  ", z,
        "P=  ",format(p, scientific = T),"\n\n")
        } 
#
Msig(mymat, mod_obs)
#
Sys.time() - TIME 
#

#Ungrazed sites
mymat<- wUG                        # [Assign here]
#
# Batch to generate the null models for modularity M significance test. ----- 
# (for each matrix, @30 min for 100 null replicates)
require(bipartite)
TIME <- Sys.time()
# Modularity for observed matrix.
# Give here the mean observed modularity values.
mod_obs <- 0.4084378              # [Assign here]
#
Msig <- function (mat, mlike)  {
    require(bipartite)
    # mat is the input matrix for which M is tested
    # mlike is the observed mean M value
    nulls <- nullmodel(mymat, N=100, method=3)
    modules.nulls <- sapply(nulls, computeModules)
    like.nulls <- sapply(modules.nulls, function(x) x@likelihood)
    z <- (mlike - mean(like.nulls))/sd(like.nulls)
    p <- 2*pnorm(-abs(z))
    cat("\n\n","P value for modularity M= ", mod_obs, "\n", "\n\n",
        "zeta=  ", z,
        "P=  ",format(p, scientific = T),"\n\n")
        } 
#
Msig(mymat, mod_obs)
#
Sys.time() - TIME 

```


```{r connectance and NODF significance}

#z = (x – μ) / σ
#The z score tells you how many standard deviations from the mean your score is. In this example, your score is 1.6 standard deviations above the mean.

require(bipartite)

mymat<- wG                        # [Assign here]

# Connectance for observed matrix.
# Give here the mean observed connectance values.

# GRAZED LANDS

con_obs <- 0.1071779           # [Assign here]
#
    require(bipartite)
    
    nulls <- nullmodel(mymat, N=100, method=3)
    null <- sapply(nulls, networklevel, index="weighted connectance")
    z <- (con_obs- mean(null))/sd(null)
    p <- 2*pnorm(-abs(z))
    cat("\n\n","P value for connectance C= ", con_obs, "\n", "\n\n",
        "zeta=  ", z,
        "P=  ",format(p, scientific = T),"\n\n")
    
    
NODF_obs<- 9.163033
  
    
    nulls <- nullmodel(mymat, N=100, method=3)
    null <- sapply(nulls, networklevel, index="weighted NODF")
    z <- (NODF_obs- mean(null))/sd(null)
    p <- 2*pnorm(-abs(z))
    cat("\n\n","P value for nestedeness N= ", NODF_obs, "\n", "\n\n",
        "zeta=  ", z,
        "P=  ",format(p, scientific = T),"\n\n")
  
   
    
# UNGRAZED LANDS
    
mymat<- wUG     

con_obs <-0.1588508    

 require(bipartite)
    
    nulls <- nullmodel(mymat, N=100, method=3)
    null <- sapply(nulls, networklevel, index="weighted connectance")
    z <- (con_obs- mean(null))/sd(null)
    p <- 2*pnorm(-abs(z))
    cat("\n\n","P value for connectance C= ", con_obs, "\n", "\n\n",
        "zeta=  ", z,
        "P=  ",format(p, scientific = T),"\n\n")
    
  
NODF_obs<- 23.68882
  
    nulls <- nullmodel(mymat, N=100, method=3)
    null <- sapply(nulls, networklevel, index="weighted NODF")
    z <- (NODF_obs- mean(null))/sd(null)
    p <- 2*pnorm(-abs(z))
    cat("\n\n","P value for nestedeness N= ", NODF_obs, "\n", "\n\n",
        "zeta=  ", z,
        "P=  ",format(p, scientific = T),"\n\n")    
      
```





### Hamming Distance, library(sna)

Is the number of addition/deletion operations required to turn the edge set of G_1 into that of G_2.The Hamming distance is extremely sensitive to nodal labeling, and should not be employed directly when nodes are interchangeable. G_1 and G_2 are labeled graphs

```{r Hamming, echo=FALSE, message= TRUE, warning= TRUE}
#-----------------------------------------------------------------------
## Comparison of two network structures.
## Based on code for MS_Integration with Alfredo and Martin.
## Sevilla. 14Mar2007. Revised July 2007 - Freiburg.
## Sevilla 30 July 2007. Added final phylogeny.
## Sevilla 12 Nov 2007. Freiburg 12-13 Nov. New version.
## Modified in Copenhagen->Aarhus train. 7 October 2011.
## Modified for Florencia analyses; Sevilla, 17 July 2017.
#-----------------------------------------------------------------------l
library(vegan)
library(ade4)
library(ca)
library(nFactors)
library(network)
library(sna)
#-----------------------------------------------------------------------
# Assign dataset, and remove trees with no interactions.
dim(ungrazed)
dim(grazed)
rowSums(ungrazed[,2:13])
rowSums(grazed[,2:13])

wUG1<-ungrazed[,2:13]    # 70x12
wG1<-(grazed[,2:13])     # 120x12

# I trimm the ungrazed dataset to delete trees with no records (9 trees).
# Trees with 0 records are not included.
# 
wUG1<- wUG1 %>% 
           dplyr::filter(rowSums(wUG1^2)!=0) 
dim(wUG1)    # 61x12

# I trimm the grazed dataset to delete trees with no records (14 trees).
# Trees with 0 records are not included.
# 
wG1<- wG1 %>% 
           dplyr::filter(rowSums(wG1^2)!=0) 
dim(wG1)    # 106x12

# I trimm the grazed dataset to delete trees so that I get N=61.
# Trees with 0 records are excluded.
# Datasets from the G and UG sites have now 61x12
rG1<- sample_n(wG1, 61, replace= F)   # Random sample from wG1 with N=70 trees (without replacement).
dim(rG1)
dim(wUG1)

#-------------------------------------------------------------------------
# sna Analyses with Quadratic assignment procedures.
# Generate graph
# NOTE: I've trimmed the adjacency matrices of the Grazed site to get the
# same dimension.
# 
# # These are the graphs
grG1<-network(rG1, vertex.attr=NULL, vertex.attrnames=NULL, directed=F,hyper=FALSE, matrix.type="adjacency", multiple=FALSE, bipartite = 61)
gwUG1<-network(wUG1, vertex.attr=NULL, vertex.attrnames=NULL, directed=F,hyper=FALSE, matrix.type="adjacency", multiple=FALSE, bipartite = 61)

#Perform qap tests of graph correlation. I use the adjacency matrices as input.
prosop<- base::array(dim=c(2,61,12))
prosop[1,,]<- as.matrix(rG1)
prosop[2,,]<- as.matrix(wUG1)

# Doesn't work when I load the graphs (!!!!)
prosop[1,,]<- grG1
prosop[2,,]<- gwUG1

#Perform qap tests of graph correlation
q<-qaptest(prosop, gcor, g1=1, g2=2)

#Examine the results
summary(q)
plot(q)

#-------------------------------------------------------------------------
# The Hamming distance between two labeled graphs G_1 and G_2 is equal to |{e : (e in E(G_1) and e not in E(G_2)) or (e not in E(G_1) and e in E(G_2))}|. In more prosaic terms, this may be thought of as the number of addition/deletion operations required to turn the edge set of G_1 into that of G_2.
# hdist
q.prosop<-qaptest(prosop,hdist,g1=1,g2=2)
summary(q.prosop)
plot(q.prosop)

# Structural Distances. structdist returns the structural distance between the labeled graphs g1 and g2 in stack dat based on Hamming distance for dichotomous data, or else the absolute (manhattan) distance. If normalize is true, this distance is divided by its dichotomous theoretical maximum (conditional on |V(G)|).
structdist(prosop,method="anneal", prob.init=0.9, prob.decay=0.85, 
    freeze.time=50, full.neighborhood=TRUE)

# Structural correlation between the adjacency matrices of graphs.
gscor(prosop)

#-----------------------------------------------------------------------

```


### Randomization tests for QAP and Hamming's distance

```{r rand_Hamming, echo=FALSE, message= TRUE, warning= TRUE}
# Fully randomized assignment of the GRAZED dataset, no. resamps: 999.
# I trimm the grazed dataset to delete trees so that I get N=61.
# Trees with 0 records are excluded.
# Datasets from the G and UG sites have now 61x12
#
TIME <- Sys.time()

rndmz<- 999                     # Number of randomizations required
rand_qobs<- NULL                # QAP q value for observed matrix in each resampling.
rand_q_mean_resamp<- NULL       # mean QAP q value for 1000 random marices of each resampling.
rand_hdist_obs<- NULL           # Hamming h value for observed matrix in each resampling.
rand_hdist_mean_resamp<- NULL   # mean Hamming h value for 1000 random marices of each resampling.
#-----------------------------
for (i in 1:rndmz) {
  rG1<- sample_n(wG1, 61, replace= F)   # Random sample from wG1 with N=70 trees (without replacement).
  #Perform qap tests of graph correlation. I use the adjacency matrices as input.
  prosop<- base::array(dim=c(2,61,12))
  prosop[1,,]<- as.matrix(rG1)
  prosop[2,,]<- as.matrix(wUG1)
  
  #Perform qap tests of graph correlation
  q<-qaptest(prosop, gcor, g1=1, g2=2)
  # Store
  rand_qobs<- append(rand_qobs,q$testval)
  rand_q_mean_resamp<- append(rand_q_mean_resamp, mean(q$dist))
  
  # hdist
  q.prosop<-qaptest(prosop,hdist,g1=1,g2=2)
  # Store
  rand_hdist_obs<- append(rand_hdist_obs, q.prosop$testval)
  rand_hdist_mean_resamp<- append(rand_hdist_mean_resamp, mean(q.prosop$dist))
}
{plot(density(rand_hdist_mean_resamp), xlim= c(min(rand_hdist_obs), max(rand_hdist_mean_resamp)))
abline(v= mean(rand_hdist_obs), col = "blue", lty = 3)}


Sys.time() - TIME

```

--------------------------

```{r session_info }
sessionInfo()

```







